# 149_World_Class_Multi_Level_Optimization_Guide.md

*World-Class Multi-Level Optimization Guide for MinZ Compiler*  
*The definitive strategy for achieving zero-overhead abstractions on 8-bit Z80 hardware*

## Executive Summary

This document presents a comprehensive optimization strategy for MinZ that achieves **near-zero runtime overhead** for modern programming abstractions on Z80 hardware. The multi-level approach spans from high-level AST transformations to low-level instruction patching, creating a **multiplicative optimization effect** where individual improvements compound to deliver **50-80% performance gains** over naive compilation.

**Key Innovation**: The combination of **TSMC (True Self-Modifying Code) instruction patching** with **aggressive register pressure optimization** creates a unique paradigm that achieves zero-overhead abstractions on resource-constrained 8-bit systems.

## 1. Optimization Architecture Overview

### 1.1 The Five-Level Pipeline
```
Source Code â†’ AST Optimizations â†’ MIR Optimizations â†’ Backend Optimizations â†’ Assembly Peephole â†’ Machine Code
     â†“              â†“                    â†“                     â†“                    â†“
  Semantic      Register Pressure    Instruction Scheduling  Pattern Matching   TSMC Patching
```

### 1.2 Optimization Philosophy
- **Early is Better**: Apply transformations when semantic information is richest
- **Compositional**: Each level builds on previous optimizations
- **Measurable**: Every optimization must prove its worth with metrics
- **Z80-Aware**: Respect the constraints and strengths of 8-bit architecture

### 1.3 Performance Target
**Goal**: Achieve **60-85% overall performance improvement** through multiplicative optimization effects:
- Register pressure reduction: 40-60% fewer spills
- TSMC instruction patching: 15-30% cycle reduction
- Constant propagation: 20-40% fewer computations
- Peephole optimization: 10-25% better instruction selection

## 2. Level 1: Frontend/AST Optimizations

### 2.1 Constant Folding and Propagation
**Goal**: Eliminate runtime computation of compile-time constants

**Implementation Location**: `minzc/pkg/semantic/constant_folder.go`

```minz
// Before optimization
const SCREEN_WIDTH: u8 = 32;
const OFFSET: u8 = 5;
let pos = SCREEN_WIDTH + OFFSET * 2;

// After AST constant folding
let pos = 42;  // Computed at compile time
```

**Implementation Strategy**:
```go
// In pkg/semantic/constant_folder.go
type ConstantFolder struct {
    constants map[string]Value
    metrics   *OptimizationMetrics
}

func (cf *ConstantFolder) FoldExpression(expr *Expression) *Expression {
    switch expr.Type {
    case BinaryOp:
        left := cf.FoldExpression(expr.Left)
        right := cf.FoldExpression(expr.Right)
        
        if left.IsConstant() && right.IsConstant() {
            // Compute at compile time
            result := cf.evaluateConstant(expr.Op, left, right)
            cf.metrics.RecordConstantFold(expr, result)
            return &Expression{Type: Literal, Value: result}
        }
    case FunctionCall:
        // Pure function calls with constant arguments
        if cf.isPureFunction(expr.Function) && cf.allArgsConstant(expr.Args) {
            result := cf.evaluateConstantCall(expr)
            cf.metrics.RecordConstantFold(expr, result)
            return &Expression{Type: Literal, Value: result}
        }
    }
    return expr
}

func (cf *ConstantFolder) evaluateConstant(op Operator, left, right Value) Value {
    switch op {
    case Add: return Value{Int: left.Int + right.Int}
    case Mul: return Value{Int: left.Int * right.Int}
    case Shl: return Value{Int: left.Int << right.Int}
    // Add more operations...
    }
}
```

### 2.2 Dead Code Elimination
**Goal**: Remove code paths that can never execute or whose results are never used

**Implementation Location**: `minzc/pkg/semantic/dead_code_eliminator.go`

```minz
// Before optimization
if false {
    expensive_computation();  // Dead code
}

let unused_var = calculate_something();  // Dead assignment

// After DCE - both eliminated
```

**Advanced Dead Code Detection**:
```go
type DeadCodeAnalyzer struct {
    usedVars    map[string]bool
    reachable   map[*Statement]bool
    sideEffects map[string]bool
    metrics     *OptimizationMetrics
}

func (dca *DeadCodeAnalyzer) AnalyzeFunction(fn *Function) {
    // 1. Mark all variable uses (backwards)
    dca.markVariableUses(fn)
    
    // 2. Mark reachable statements (forwards)
    dca.markReachableStatements(fn)
    
    // 3. Analyze side effects
    dca.analyzeSideEffects(fn)
    
    // 4. Remove unreachable statements and unused assignments
    originalSize := len(fn.Body.Statements)
    fn.Body = dca.eliminateDeadCode(fn.Body)
    
    eliminated := originalSize - len(fn.Body.Statements)
    dca.metrics.RecordDeadCodeElimination(eliminated)
}

func (dca *DeadCodeAnalyzer) markVariableUses(fn *Function) {
    // Backwards analysis to find all used variables
    for i := len(fn.Body.Statements) - 1; i >= 0; i-- {
        stmt := fn.Body.Statements[i]
        dca.markUsesInStatement(stmt)
    }
}
```

### 2.3 Loop Optimizations
**Goal**: Transform loops for optimal register usage and minimal overhead

**Implementation Location**: `minzc/pkg/semantic/loop_optimizer.go`

```minz
// Before: Loop with invariant computation
for i in 0..n {
    base_addr = SCREEN_BASE + ROW_SIZE;  // Invariant!
    pixel = screen[base_addr + i];
    process(pixel);
}

// After: Loop-invariant code motion
base_addr = SCREEN_BASE + ROW_SIZE;  // Moved outside
for i in 0..n {
    pixel = screen[base_addr + i];
    process(pixel);
}
```

**Implementation Strategy**:
```go
type LoopOptimizer struct {
    invariants map[*Expression]bool
    metrics    *OptimizationMetrics
}

func (lo *LoopOptimizer) OptimizeLoop(loop *ForStatement) {
    // 1. Identify loop-invariant expressions
    invariants := lo.findInvariants(loop)
    
    // 2. Move invariants outside the loop
    moved := lo.hoistInvariants(loop, invariants)
    lo.metrics.RecordInvariantHoisting(moved)
    
    // 3. Strength reduction (multiply â†’ add)
    reduced := lo.strengthReduction(loop)
    lo.metrics.RecordStrengthReduction(reduced)
    
    // 4. Loop unrolling for small, fixed bounds
    if lo.shouldUnroll(loop) {
        lo.unrollLoop(loop)
        lo.metrics.RecordLoopUnrolling(loop)
    }
}
```

## 3. Level 2: Middle-End/MIR Optimizations

### 3.1 Register Pressure Optimization
**Goal**: Minimize the number of values alive simultaneously

**Implementation Location**: `minzc/pkg/optimizer/register_pressure.go`

```go
type RegisterPressureOptimizer struct {
    liveness    map[Register]LiveRange
    spillCosts  map[Register]int
    metrics     *OptimizationMetrics
}

func (rpo *RegisterPressureOptimizer) OptimizeFunction(fn *Function) {
    // 1. Build liveness analysis
    liveness := rpo.buildLivenessAnalysis(fn)
    
    // 2. Identify high-pressure regions
    criticalRegions := rpo.findCriticalRegions(liveness)
    
    // 3. Apply pressure-reducing transformations
    originalSpills := rpo.countSpills(fn)
    
    for _, region := range criticalRegions {
        rpo.reducePressure(region)
    }
    
    newSpills := rpo.countSpills(fn)
    rpo.metrics.RecordSpillReduction(originalSpills - newSpills)
}

func (rpo *RegisterPressureOptimizer) reducePressure(region *BasicBlock) {
    // Strategy 1: Reorder instructions to minimize live ranges
    region.Instructions = rpo.scheduleForMinimalPressure(region.Instructions)
    
    // Strategy 2: Split long live ranges at call boundaries
    rpo.splitLiveRangesAtCalls(region)
    
    // Strategy 3: Rematerialize cheap values instead of spilling
    rpo.rematerializeInsteadOfSpill(region)
    
    // Strategy 4: Use dependency barriers for register reuse
    rpo.insertRegisterResetPoints(region)
}

func (rpo *RegisterPressureOptimizer) scheduleForMinimalPressure(instructions []MIRInstruction) []MIRInstruction {
    // Build dependency graph
    deps := rpo.buildDependencyGraph(instructions)
    
    // Schedule independent operations first
    scheduled := []MIRInstruction{}
    ready := rpo.findReadyInstructions(deps)
    
    for len(ready) > 0 {
        // Pick instruction that minimizes pressure
        best := rpo.selectBestInstruction(ready)
        scheduled = append(scheduled, best)
        
        // Update ready set
        ready = rpo.updateReadySet(deps, scheduled)
    }
    
    return scheduled
}
```

### 3.2 Instruction Scheduling
**Goal**: Reorder instructions to minimize register pressure and maximize efficiency

```mir
// Before: Poor scheduling - 4 registers alive
r1 = load a       ; r1 becomes live
r2 = load b       ; r2 becomes live  
r3 = load c       ; r3 becomes live
r4 = r1 + r2      ; r4 becomes live (4 registers alive!)
call func(r3)     ; r3 dies
r5 = r4 + const   ; Error: too much pressure!

// After: Optimal scheduling - max 2 registers alive
r1 = load a       ; r1 becomes live
r2 = load b       ; r2 becomes live
r4 = r1 + r2      ; r1, r2 die; r4 becomes live
r3 = load c       ; r3 becomes live (only 2 registers alive)
call func(r3)     ; r3 dies
r5 = r4 + const   ; r4 dies, r5 becomes live
```

### 3.3 Constant Propagation at MIR Level
**Implementation Location**: `minzc/pkg/optimizer/mir_constant_propagation.go`

```go
type MIRConstantPropagator struct {
    constants map[Register]Value
    metrics   *OptimizationMetrics
}

func (mcp *MIRConstantPropagator) PropagateConstants(fn *Function) {
    for _, block := range fn.Blocks {
        for i, inst := range block.Instructions {
            switch inst.Op {
            case LoadConst:
                // Track constant values
                mcp.constants[inst.Dest] = inst.Value
                
            case Add:
                // Try to fold constant operations
                if leftVal, ok := mcp.constants[inst.Left]; ok {
                    if rightVal, ok := mcp.constants[inst.Right]; ok {
                        // Both operands are constants - fold!
                        result := leftVal + rightVal
                        block.Instructions[i] = MIRInstruction{
                            Op: LoadConst,
                            Dest: inst.Dest,
                            Value: result,
                        }
                        mcp.constants[inst.Dest] = result
                        mcp.metrics.RecordConstantFold()
                    }
                }
                
            case Call:
                // Function calls may invalidate constants (conservative)
                if !mcp.isPureFunction(inst.Target) {
                    mcp.invalidateMemoryConstants()
                }
            }
        }
    }
}
```

## 4. Level 3: Backend/Z80 Optimizations

### 4.1 Register Allocation with Graph Coloring
**Goal**: Optimally assign virtual registers to physical Z80 registers

**Implementation Location**: `minzc/pkg/codegen/register_allocator.go`

```go
type RegisterAllocator struct {
    interferenceGraph *InterferenceGraph
    z80Registers     []PhysicalRegister
    allocation       map[VirtualRegister]PhysicalRegister
    metrics          *OptimizationMetrics
}

var Z80_REGISTERS = []PhysicalRegister{
    {Name: "A", Type: GP8, Cost: 1},    // Accumulator - cheapest
    {Name: "B", Type: GP8, Cost: 2},    // General purpose
    {Name: "C", Type: GP8, Cost: 2},
    {Name: "D", Type: GP8, Cost: 2},
    {Name: "E", Type: GP8, Cost: 2},
    {Name: "H", Type: GP8, Cost: 2},
    {Name: "L", Type: GP8, Cost: 2},
    {Name: "IX", Type: Index16, Cost: 5}, // Expensive - avoid
    {Name: "IY", Type: Index16, Cost: 5},
}

func (ra *RegisterAllocator) AllocateRegisters(fn *Function) {
    // 1. Build interference graph
    ra.interferenceGraph = ra.buildInterferenceGraph(fn)
    
    // 2. Compute spill costs based on usage frequency
    spillCosts := ra.computeSpillCosts(fn)
    
    // 3. Color the graph using Chaitin's algorithm
    coloring := ra.colorGraph(ra.interferenceGraph, spillCosts)
    
    // 4. Handle spills if necessary
    if !coloring.isComplete() {
        spilledNodes := coloring.getSpilledNodes()
        ra.insertSpillCode(fn, spilledNodes)
        ra.metrics.RecordSpills(len(spilledNodes))
        return ra.AllocateRegisters(fn) // Retry with spill code
    }
    
    // 5. Apply the successful allocation
    ra.rewriteInstructions(fn, coloring)
    ra.metrics.RecordSuccessfulAllocation(len(ra.interferenceGraph.Nodes))
}

func (ra *RegisterAllocator) computeSpillCosts(fn *Function) map[VirtualRegister]float64 {
    costs := make(map[VirtualRegister]float64)
    
    for reg := range ra.interferenceGraph.Nodes {
        // Cost = (uses + defines) / live_range_length
        useDefs := ra.countUsesDefs(fn, reg)
        liveRange := ra.getLiveRangeLength(fn, reg)
        
        // Registers used in loops are more expensive to spill
        loopNesting := ra.getLoopNestingLevel(fn, reg)
        loopFactor := math.Pow(10, float64(loopNesting))
        
        costs[reg] = (float64(useDefs) * loopFactor) / float64(liveRange)
    }
    
    return costs
}
```

### 4.2 Z80-Specific Peephole Patterns
**Goal**: Apply architecture-specific micro-optimizations

**Implementation Location**: `minzc/pkg/codegen/z80_peephole.go`

```go
var z80PeepholePatterns = []PeepholePattern{
    // Pattern: LD A, 0 â†’ XOR A (1 byte smaller, 4 cycles faster)
    {
        Name: "zero_load_optimization",
        Match: []string{"LD A, #0"},
        Replace: []string{"XOR A"},
        Savings: PeepholeSavings{Cycles: 4, Bytes: 1},
        Conditions: []Condition{NotAffectingOtherFlags},
    },
    
    // Pattern: ADD A, 1 â†’ INC A (3 cycles faster)
    {
        Name: "increment_optimization",
        Match: []string{"ADD A, #1"},
        Replace: []string{"INC A"},
        Savings: PeepholeSavings{Cycles: 3, Bytes: 1},
    },
    
    // Pattern: Multiple INC A â†’ ADD A, n (when n > 3)
    {
        Name: "multiple_increment_optimization",
        Match: []string{"INC A", "INC A", "INC A", "INC A"},
        Replace: []string{"ADD A, #4"},
        Savings: PeepholeSavings{Cycles: 4, Bytes: 2},
        Conditions: []Condition{CountGreaterThan(3)},
    },
    
    // Pattern: CP 0 â†’ OR A (test for zero, 4 cycles faster)
    {
        Name: "zero_test_optimization", 
        Match: []string{"CP #0"},
        Replace: []string{"OR A"},
        Savings: PeepholeSavings{Cycles: 4, Bytes: 1},
    },
    
    // Pattern: LD reg, #0 â†’ XOR reg (for B,C,D,E)
    {
        Name: "register_zero_optimization",
        Match: []string{"LD {reg}, #0"},
        Replace: []string{"XOR {reg}"},
        Savings: PeepholeSavings{Cycles: 3, Bytes: 1},
        Conditions: []Condition{RegisterIn("B", "C", "D", "E")},
    },
    
    // Pattern: 16-bit compare optimization
    {
        Name: "16bit_zero_compare",
        Match: []string{"LD A, H", "OR L", "CP #0"},
        Replace: []string{"LD A, H", "OR L"},
        Savings: PeepholeSavings{Cycles: 4, Bytes: 1},
        Comment: "OR sets Z flag for 16-bit zero test",
    },
}

type PeepholeOptimizer struct {
    patterns []PeepholePattern
    metrics  *OptimizationMetrics
}

func (po *PeepholeOptimizer) ApplyPeepholeOptimizations(instructions []Instruction) []Instruction {
    optimized := make([]Instruction, 0, len(instructions))
    i := 0
    
    for i < len(instructions) {
        matched := false
        
        // Try each pattern
        for _, pattern := range po.patterns {
            if po.matchesPattern(instructions[i:], pattern) {
                // Apply the optimization
                replacement := po.applyPattern(instructions[i:], pattern)
                optimized = append(optimized, replacement...)
                
                // Record the optimization
                po.metrics.RecordPeepholeOptimization(pattern)
                
                i += len(pattern.Match)
                matched = true
                break
            }
        }
        
        if !matched {
            optimized = append(optimized, instructions[i])
            i++
        }
    }
    
    return optimized
}
```

### 4.3 Shadow Register Optimization
**Goal**: Utilize Z80's shadow register set for performance

**Implementation Location**: `minzc/pkg/codegen/z80_shadow_registers.go`

```go
type ShadowRegisterOptimizer struct {
    shadowUsage map[Function]bool
    metrics     *OptimizationMetrics
}

func (sro *ShadowRegisterOptimizer) OptimizeShadowRegisterUsage(fn *Function) {
    // Identify functions suitable for shadow register optimization
    if sro.shouldUseShadowRegisters(fn) {
        sro.insertShadowRegisterSwaps(fn)
        sro.metrics.RecordShadowRegisterOptimization()
    }
}

func (sro *ShadowRegisterOptimizer) shouldUseShadowRegisters(fn *Function) bool {
    return fn.IsInterruptHandler ||
           fn.HasHighRegisterPressure() ||
           fn.HasExpensiveFunctionCalls()
}

func (sro *ShadowRegisterOptimizer) insertShadowRegisterSwaps(fn *Function) {
    // Pattern: Preserve registers around function calls
    for _, call := range fn.CallSites {
        if sro.shouldPreserveAroundCall(call) {
            // Insert EXX before call to preserve BC, DE, HL
            preserveInst := Instruction{
                Op: "EXX",
                Comment: "Switch to shadow registers for call",
                Cycles: 4,
            }
            
            // Insert EXX after call to restore
            restoreInst := Instruction{
                Op: "EXX", 
                Comment: "Restore main registers",
                Cycles: 4,
            }
            
            fn.insertBefore(call, preserveInst)
            fn.insertAfter(call, restoreInst)
        }
    }
}

// Example output:
func generateWithShadowRegisters() string {
    return `
    ; Original: expensive preservation
    PUSH BC          ; 11 cycles, 1 byte
    PUSH DE          ; 11 cycles, 1 byte  
    PUSH HL          ; 11 cycles, 1 byte
    CALL expensive_func
    POP HL           ; 10 cycles, 1 byte
    POP DE           ; 10 cycles, 1 byte
    POP BC           ; 10 cycles, 1 byte
    ; Total: 64 cycles, 6 bytes
    
    ; Optimized: shadow register swap
    EXX              ; 4 cycles, 1 byte
    CALL expensive_func
    EXX              ; 4 cycles, 1 byte  
    ; Total: 8 cycles, 2 bytes - 87% faster!
    `
}
```

## 5. Level 4: Advanced TSMC Optimization System

### 5.1 Smart Patch Point Generation
**Goal**: Generate optimal self-modifying code patterns

**Implementation Location**: `minzc/pkg/optimizer/tsmc_optimizer.go`

```go
type TSMCOptimizer struct {
    patchPoints map[string]PatchPoint
    templates   map[string]CodeTemplate
    metrics     *OptimizationMetrics
}

type PatchPoint struct {
    Label       string
    Type        PatchType  // IMMEDIATE, BRANCH, OPCODE
    Size        int        // Bytes that can be patched
    Constraints []string   // Architectural constraints
    Frequency   int        // How often this gets patched
}

func (tsmc *TSMCOptimizer) GeneratePatchPoints(fn *Function) {
    for _, call := range fn.CallSites {
        target := call.Target
        
        if tsmc.shouldUseTSMC(target, call) {
            usage := tsmc.analyzeUsagePattern(call)
            patchType := tsmc.determinePatchType(call, usage)
            
            switch patchType {
            case ImmediateReturn:
                // Generate NOPâ†’RET patch point for immediate use
                tsmc.generateImmediateReturnPatch(call, usage)
                tsmc.metrics.RecordTSMCPatch("immediate_return")
                
            case ParameterPatching:
                // Generate parameter injection patches
                tsmc.generateParameterPatches(call, usage)
                tsmc.metrics.RecordTSMCPatch("parameter_injection")
                
            case BehavioralMorphing:
                // Generate opcode morphing patches
                tsmc.generateOpcodePatches(call, usage)
                tsmc.metrics.RecordTSMCPatch("behavioral_morphing")
            }
        }
    }
}

func (tsmc *TSMCOptimizer) generateImmediateReturnPatch(call *CallSite, usage UsagePattern) {
    // Create the patch point with NOPâ†’RET transformation
    patchPoint := PatchPoint{
        Label: fmt.Sprintf("%s_immediate_patch", call.Target.Name),
        Type: OpcodePatch,
        Size: 1,
        Constraints: []string{"single_byte_opcode"},
    }
    
    // Generate the patch template
    template := CodeTemplate{
        Name: "immediate_return_template",
        Code: fmt.Sprintf(`
%s:
%s.op:
%s equ %s.op + 1
    NOP              ; PATCH POINT: NOP(0x00) â†’ RET(0xC9)
    LD A, #%d        ; Return value (patchable)
    RET              ; Fallback return
`, call.Target.Name, patchPoint.Label, patchPoint.Label, patchPoint.Label, usage.ConstantResult),
        PatchPoints: []PatchPoint{patchPoint},
        Savings: PatchSavings{Cycles: 24, Description: "Skip function body for immediate returns"},
    }
    
    tsmc.templates[call.Target.Name] = template
}
```

### 5.2 Usage Pattern Analysis
**Goal**: Detect optimal patching strategies based on usage patterns

```go
type UsagePattern struct {
    CallSite        *CallSite
    ResultUsage     ResultUsageType
    Frequency       int
    ConstantArgs    []Value
    ConstantResult  Value
    Context         CallContext
}

type ResultUsageType int
const (
    ImmediateUse     ResultUsageType = iota  // temp = func(); use(temp)
    StoredResult                             // result = func(); later: use(result)
    DiscardedResult                          // func(); // result unused
    ConditionalUse                           // if func() > 0 { ... }
)

func (analyzer *UsageAnalyzer) AnalyzeCallPattern(call *CallSite) UsagePattern {
    // Analyze the next few instructions after the call
    nextInsts := analyzer.getNextInstructions(call, 5)
    
    pattern := UsagePattern{
        CallSite: call,
        Frequency: analyzer.getCallFrequency(call),
        Context: analyzer.getCallContext(call),
    }
    
    // Detect constant arguments
    pattern.ConstantArgs = analyzer.getConstantArguments(call)
    
    // Analyze result usage pattern
    if analyzer.isImmediateUse(nextInsts) {
        pattern.ResultUsage = ImmediateUse
        // Perfect candidate for NOPâ†’RET patching!
        
        // If result is constant, compute it now
        if len(pattern.ConstantArgs) == len(call.Args) {
            pattern.ConstantResult = analyzer.computeConstantCall(call, pattern.ConstantArgs)
        }
        
    } else if analyzer.isStoredForLater(nextInsts) {
        pattern.ResultUsage = StoredResult
        // Use parameter patching + store template
        
    } else if analyzer.isResultDiscarded(nextInsts) {
        pattern.ResultUsage = DiscardedResult
        // Dead code elimination candidate
    }
    
    return pattern
}

func (analyzer *UsageAnalyzer) computeConstantCall(call *CallSite, args []Value) Value {
    // For pure functions with constant arguments, compute result at compile time
    if analyzer.isPureFunction(call.Target) {
        switch call.Target.Name {
        case "add":
            return Value{Int: args[0].Int + args[1].Int}
        case "mul":
            return Value{Int: args[0].Int * args[1].Int}
        case "shl":
            return Value{Int: args[0].Int << args[1].Int}
        // Add more pure functions...
        }
    }
    return Value{} // Unknown result
}
```

### 5.3 Advanced Instruction Patching
**Goal**: Implement sophisticated code morphing patterns

```go
// Revolutionary single-byte opcode patching
type OpcodePatch struct {
    Name           string
    OriginalOpcode byte
    PatchedOpcode  byte
    Condition      PatchCondition
    Savings        PatchSavings
}

type PatchSavings struct {
    Cycles      int
    Bytes       int
    Description string
}

var advancedPatchPatterns = map[string]OpcodePatch{
    // NOP â†’ RET for immediate returns (saves 20-30 T-states)
    "immediate_return": {
        Name:           "immediate_return_optimization",
        OriginalOpcode: 0x00, // NOP
        PatchedOpcode:  0xC9, // RET
        Condition:      OnImmediateUse,
        Savings:        PatchSavings{Cycles: 24, Description: "Skip function execution"},
    },
    
    // NOP â†’ JR for conditional branching
    "conditional_branch": {
        Name:           "conditional_branch_optimization", 
        OriginalOpcode: 0x00, // NOP
        PatchedOpcode:  0x18, // JR n
        Condition:      OnCondition,
        Savings:        PatchSavings{Cycles: 12, Description: "Direct branch"},
    },
    
    // LD A, n â†’ XOR A for zero values (saves 4 T-states, 1 byte)
    "zero_optimization": {
        Name:           "zero_value_optimization",
        OriginalOpcode: 0x3E, // LD A, n
        PatchedOpcode:  0xAF, // XOR A
        Condition:      OnZeroValue,
        Savings:        PatchSavings{Cycles: 4, Bytes: 1, Description: "Zero via XOR"},
    },
    
    // ADD A, n â†’ INC A for value 1 (saves 3 T-states)
    "increment_optimization": {
        Name:           "increment_by_one_optimization",
        OriginalOpcode: 0xC6, // ADD A, n  
        PatchedOpcode:  0x3C, // INC A
        Condition:      OnValueOne,
        Savings:        PatchSavings{Cycles: 3, Bytes: 1, Description: "Increment instead of add"},
    },
}

func (patcher *InstructionPatcher) ApplyRuntimePatch(address uint16, patch OpcodePatch) {
    // Verify patch is safe to apply
    if !patcher.verifyPatchSafety(address, patch) {
        patcher.logPatchFailure(address, patch, "Safety check failed")
        return
    }
    
    // Apply the single-byte patch
    oldOpcode := patcher.memory.ReadByte(address)
    patcher.memory.WriteByte(address, patch.PatchedOpcode)
    
    // Log the patch for analysis
    patcher.logPatchApplication(address, oldOpcode, patch)
    
    // Update metrics
    patcher.metrics.RecordRuntimePatch(patch)
}
```

## 6. Level 5: Assembly-Level Micro-Optimizations

### 6.1 Instruction Selection Optimization
**Goal**: Choose the most efficient instruction sequence for each operation

**Implementation Location**: `minzc/pkg/codegen/instruction_selector.go`

```go
type InstructionSelector struct {
    patterns map[OperationType][]InstructionPattern
    metrics  *OptimizationMetrics
}

type InstructionPattern struct {
    Name        string
    Pattern     []string
    Cycles      int
    Bytes       int
    Constraints []Constraint
    Conditions  []Condition
}

func (is *InstructionSelector) SelectOptimalInstructions(operation *Operation) []Instruction {
    candidates := is.patterns[operation.Type]
    
    var bestPattern *InstructionPattern
    bestScore := math.MaxInt32
    
    for _, pattern := range candidates {
        if is.satisfiesConstraints(pattern, operation) {
            // Score based on cycles and bytes (cycles weighted higher)
            score := pattern.Cycles*4 + pattern.Bytes // Prefer speed over size
            
            if score < bestScore {
                bestScore = score
                bestPattern = &pattern
            }
        }
    }
    
    if bestPattern != nil {
        is.metrics.RecordInstructionSelection(operation.Type, bestPattern.Name)
        return is.generateInstructions(bestPattern, operation)
    }
    
    // Fallback to default pattern
    return is.generateDefault(operation)
}

// Example patterns for u8 addition:
var addU8Patterns = []InstructionPattern{
    {
        Name:        "register_add",
        Pattern:     []string{"ADD A, {reg}"},
        Cycles:      4,
        Bytes:       1,
        Constraints: []Constraint{BothInRegisters},
    },
    {
        Name:        "immediate_add", 
        Pattern:     []string{"ADD A, #{val}"},
        Cycles:      7,
        Bytes:       2,
        Constraints: []Constraint{OneConstant},
    },
    {
        Name:        "increment",
        Pattern:     []string{"INC A"},
        Cycles:      4,
        Bytes:       1,
        Constraints: []Constraint{AddingOne},
        Conditions:  []Condition{DestinationIsA},
    },
    {
        Name:        "decrement",
        Pattern:     []string{"DEC A"},
        Cycles:      4,
        Bytes:       1, 
        Constraints: []Constraint{SubtractingOne},
        Conditions:  []Condition{DestinationIsA},
    },
}
```

### 6.2 Branch Optimization
**Goal**: Optimize conditional branches for common cases

**Implementation Location**: `minzc/pkg/codegen/branch_optimizer.go`

```go
type BranchOptimizer struct {
    branchStats map[string]BranchStatistics
    metrics     *OptimizationMetrics
}

type BranchStatistics struct {
    TakenCount    int
    NotTakenCount int
    Probability   float64
}

func (bo *BranchOptimizer) OptimizeBranches(fn *Function) {
    for _, block := range fn.BasicBlocks {
        for i, inst := range block.Instructions {
            if inst.IsBranch() {
                optimized := bo.optimizeBranch(inst)
                if optimized != nil {
                    block.Instructions[i] = *optimized
                    bo.metrics.RecordBranchOptimization(inst.Op, optimized.Op)
                }
            }
        }
    }
}

func (bo *BranchOptimizer) optimizeBranch(branch *Instruction) *Instruction {
    // Optimize: if (x == 0) â†’ test with OR A/CP 0 â†’ OR A
    if branch.IsCompareWithZero() {
        return &Instruction{
            Op: "OR A",
            Comment: "Optimized zero comparison",
            Cycles: branch.Cycles - 3,
            Bytes: branch.Bytes - 1,
        }
    }
    
    // Optimize: Invert condition if fall-through is more likely
    stats := bo.branchStats[branch.Label]
    if stats.Probability < 0.3 { // Usually not taken
        return bo.invertBranch(branch)
    }
    
    // Optimize: Use shorter jumps when possible
    if bo.isShortJumpCandidate(branch) {
        return bo.convertToShortJump(branch)
    }
    
    return nil
}

func (bo *BranchOptimizer) invertBranch(branch *Instruction) *Instruction {
    // Convert JP Z, label â†’ JP NZ, fallthrough; label:
    inverted := &Instruction{
        Op:      bo.invertCondition(branch.Op),
        Label:   branch.FallThrough,
        Comment: "Inverted branch for better prediction",
        Cycles:  branch.Cycles,
        Bytes:   branch.Bytes,
    }
    
    return inverted
}
```

## 7. Performance Measurement Framework

### 7.1 Optimization Metrics
**Implementation Location**: `minzc/pkg/metrics/optimization_metrics.go`

```go
type OptimizationMetrics struct {
    // Cycle savings by optimization type
    CyclesSavedConstantFolding     int
    CyclesSavedRegisterOptimization int
    CyclesSavedTSMCPatching       int
    CyclesSavedPeephole           int
    
    // Code size changes
    BytesSavedConstantPropagation int
    BytesSavedDeadCodeElimination int
    BytesSavedPeephole           int
    
    // Register allocation metrics
    RegisterSpillsEliminated     int
    RegisterPressureReductions   int
    ShadowRegisterOptimizations  int
    
    // TSMC statistics
    PatchPointsGenerated        int
    RuntimePatchesApplied       int
    TSMCOptimizationsSuccessful int
    
    // Overall measurements
    BaselineCycles             int
    OptimizedCycles           int
    BaselineBytes             int
    OptimizedBytes            int
    
    // Timing
    CompilationTimeMs         int64
    OptimizationTimeMs        int64
}

func (metrics *OptimizationMetrics) RecordOptimization(opt OptimizationType, savings OptimizationSavings) {
    switch opt {
    case ConstantFolding:
        metrics.CyclesSavedConstantFolding += savings.Cycles
        metrics.BytesSavedConstantPropagation += savings.Bytes
        
    case RegisterPressure:
        metrics.CyclesSavedRegisterOptimization += savings.Cycles
        metrics.RegisterSpillsEliminated += savings.SpillsEliminated
        
    case TSMCPatching:
        metrics.CyclesSavedTSMCPatching += savings.Cycles
        metrics.RuntimePatchesApplied += savings.PatchesApplied
        
    case PeepholeOptimization:
        metrics.CyclesSavedPeephole += savings.Cycles
        metrics.BytesSavedPeephole += savings.Bytes
    }
}

func (metrics *OptimizationMetrics) GenerateReport() OptimizationReport {
    totalCyclesSaved := metrics.CyclesSavedConstantFolding +
                       metrics.CyclesSavedRegisterOptimization +
                       metrics.CyclesSavedTSMCPatching +
                       metrics.CyclesSavedPeephole
    
    totalBytesSaved := metrics.BytesSavedConstantPropagation +
                      metrics.BytesSavedDeadCodeElimination +
                      metrics.BytesSavedPeephole
    
    return OptimizationReport{
        // Performance improvements
        TotalCyclesSaved: totalCyclesSaved,
        PerformanceGain: float64(totalCyclesSaved) / float64(metrics.BaselineCycles) * 100,
        
        // Size improvements  
        TotalBytesSaved: totalBytesSaved,
        CodeSizeReduction: float64(totalBytesSaved) / float64(metrics.BaselineBytes) * 100,
        
        // Optimization breakdown
        BreakdownByCycles: map[string]int{
            "Constant Folding":      metrics.CyclesSavedConstantFolding,
            "Register Optimization": metrics.CyclesSavedRegisterOptimization,
            "TSMC Patching":        metrics.CyclesSavedTSMCPatching,
            "Peephole":             metrics.CyclesSavedPeephole,
        },
        
        // Success rates
        OptimizationSuccessRate: float64(metrics.TSMCOptimizationsSuccessful) / 
                                float64(metrics.PatchPointsGenerated) * 100,
        
        // Compilation overhead
        CompilationOverhead: float64(metrics.OptimizationTimeMs) / 
                            float64(metrics.CompilationTimeMs) * 100,
    }
}
```

### 7.2 Benchmark Suite
**Implementation Location**: `minzc/pkg/benchmarks/optimization_benchmarks.go`

```go
type BenchmarkSuite struct {
    benchmarks []Benchmark
    baselines  map[string]Performance
    metrics    *BenchmarkMetrics
}

type Benchmark struct {
    Name            string
    Source          string
    ExpectedCycles  int
    ExpectedBytes   int
    Category        BenchmarkCategory
}

type BenchmarkCategory int
const (
    ArithmeticBenchmark BenchmarkCategory = iota
    LoopBenchmark
    FunctionCallBenchmark
    MemoryAccessBenchmark
    ControlFlowBenchmark
)

var optimizationBenchmarks = []Benchmark{
    {
        Name: "constant_folding_arithmetic",
        Source: `
            const A: u8 = 10;
            const B: u8 = 20;  
            fun test() -> u8 {
                return A + B * 2; // Should become: return 50;
            }
        `,
        ExpectedCycles: 10, // Just return constant
        ExpectedBytes: 5,
        Category: ArithmeticBenchmark,
    },
    
    {
        Name: "register_pressure_loop",
        Source: `
            fun sum_array(arr: [u8; 10]) -> u16 {
                let sum: u16 = 0;
                for i in 0..10 {
                    sum += arr[i] as u16;
                }
                return sum;
            }
        `,
        ExpectedCycles: 150, // Optimized loop
        ExpectedBytes: 25,
        Category: LoopBenchmark,
    },
    
    {
        Name: "tsmc_function_calls",
        Source: `
            fun add_pure(a: u8, b: u8) -> u8 { return a + b; }
            fun test() -> u8 {
                let temp = add_pure(10, 20); // Should patch to immediate return
                return temp;
            }
        `,
        ExpectedCycles: 25, // Should skip function body
        ExpectedBytes: 8,
        Category: FunctionCallBenchmark,
    },
    
    {
        Name: "peephole_optimizations",
        Source: `
            fun test_peephole(x: u8) -> u8 {
                if x == 0 {  // Should use OR A instead of CP 0
                    return x + 1;  // Should use INC A
                }
                return x;
            }
        `,
        ExpectedCycles: 20,
        ExpectedBytes: 12,
        Category: ControlFlowBenchmark,
    },
}

func (suite *BenchmarkSuite) RunOptimizationBenchmarks() BenchmarkResults {
    results := BenchmarkResults{
        Results: make(map[string]BenchmarkResult),
    }
    
    for _, benchmark := range suite.benchmarks {
        // Compile without optimizations (baseline)
        unoptimized := suite.compileWithFlags(benchmark.Source, CompilerFlags{
            OptimizationLevel: 0,
            EnableTSMC:       false,
            EnablePeephole:   false,
        })
        
        // Compile with full optimization pipeline
        optimized := suite.compileWithFlags(benchmark.Source, CompilerFlags{
            OptimizationLevel: 3,
            EnableTSMC:       true,
            EnablePeephole:   true,
            EnableConstantFolding: true,
            EnableRegisterOptimization: true,
        })
        
        // Measure improvements
        improvement := BenchmarkResult{
            Name: benchmark.Name,
            BaselineCycles: unoptimized.Cycles,
            OptimizedCycles: optimized.Cycles,
            BaselineBytes: unoptimized.Bytes,
            OptimizedBytes: optimized.Bytes,
            
            CycleImprovement: float64(unoptimized.Cycles - optimized.Cycles) / 
                            float64(unoptimized.Cycles) * 100,
            SizeImprovement: float64(unoptimized.Bytes - optimized.Bytes) /
                           float64(unoptimized.Bytes) * 100,
            
            MetExpectations: optimized.Cycles <= benchmark.ExpectedCycles &&
                           optimized.Bytes <= benchmark.ExpectedBytes,
        }
        
        // Verify correctness
        if !suite.verifyCorrectness(unoptimized, optimized) {
            improvement.CorrectnessError = "Optimization changed program semantics"
        }
        
        results.Results[benchmark.Name] = improvement
        suite.metrics.RecordBenchmark(improvement)
    }
    
    return results
}

func (suite *BenchmarkSuite) verifyCorrectness(unoptimized, optimized CompilationResult) bool {
    // Run both versions with same inputs and compare outputs
    testInputs := suite.generateTestInputs(10)
    
    for _, input := range testInputs {
        unoptResult := suite.executeProgram(unoptimized, input)
        optResult := suite.executeProgram(optimized, input)
        
        if !unoptResult.equals(optResult) {
            suite.logCorrectnessError(input, unoptResult, optResult)
            return false
        }
    }
    
    return true
}
```

## 8. Implementation Roadmap

### 8.1 Phase 1: Foundation (Immediate - 1-2 weeks)
**Priority**: Critical infrastructure
**Location**: `minzc/pkg/metrics/`, `minzc/pkg/benchmarks/`

- [ ] **Implement optimization metrics framework**
  - Create `OptimizationMetrics` struct and methods
  - Add metrics collection throughout existing pipeline
  - Implement report generation

- [ ] **Set up benchmark suite**
  - Create test cases for each optimization type
  - Implement correctness verification
  - Add performance measurement infrastructure

- [ ] **Add basic liveness analysis**
  - Implement live range calculation
  - Create interference graph builder
  - Add register pressure measurement

### 8.2 Phase 2: Frontend Optimizations (2-3 weeks)
**Priority**: High impact, foundational
**Location**: `minzc/pkg/semantic/`

- [ ] **Implement constant folding at AST level**
  - Binary expression evaluation
  - Function call constant propagation
  - Array index constant calculation

- [ ] **Add dead code elimination**
  - Unreachable code detection
  - Unused variable elimination
  - Pure function call removal

- [ ] **Implement loop optimizations**
  - Loop-invariant code motion
  - Strength reduction (multiply â†’ add)
  - Small loop unrolling

### 8.3 Phase 3: Register Optimization (3-4 weeks)
**Priority**: Maximum performance impact
**Location**: `minzc/pkg/optimizer/`

- [ ] **Implement register pressure analysis**
  - Build comprehensive liveness analysis
  - Identify high-pressure regions
  - Create spill cost calculation

- [ ] **Add instruction scheduling**
  - Dependency graph construction
  - Critical path analysis
  - Pressure-aware instruction reordering

- [ ] **Implement graph-coloring register allocation**
  - Chaitin's algorithm adaptation for Z80
  - Spill code generation
  - Register coalescing

### 8.4 Phase 4: TSMC Enhancement (3-4 weeks) 
**Priority**: Revolutionary feature completion
**Location**: `minzc/pkg/optimizer/`

- [ ] **Enhance usage pattern analysis**
  - Call site analysis improvement
  - Result usage detection
  - Constant argument tracking

- [ ] **Implement smart patch point generation**
  - Template system creation
  - Patch safety verification
  - Runtime patch application

- [ ] **Add advanced instruction patching**
  - Single-byte opcode morphing
  - Behavioral pattern templates
  - Patch effectiveness measurement

### 8.5 Phase 5: Backend Optimization (2-3 weeks)
**Priority**: Architecture-specific gains
**Location**: `minzc/pkg/codegen/`

- [ ] **Implement comprehensive peephole patterns**
  - Expand pattern library to 50+ patterns
  - Add Z80-specific optimizations
  - Implement pattern matching engine

- [ ] **Add shadow register optimization**
  - Function call register preservation
  - Interrupt handler optimization
  - Register pressure relief

- [ ] **Create instruction selection framework**
  - Multi-pattern instruction selection
  - Cost-based pattern choosing
  - Architecture-aware optimization

### 8.6 Phase 6: Integration & Tuning (2-3 weeks)
**Priority**: Final polish and validation
**Location**: All packages

- [ ] **Integrate all optimization levels**
  - Ensure proper phase ordering
  - Handle inter-phase dependencies
  - Optimize compilation pipeline

- [ ] **Fine-tune optimization parameters**
  - Adjust spill cost calculations
  - Tune peephole pattern priorities
  - Calibrate TSMC heuristics

- [ ] **Add comprehensive benchmarking**
  - Real-world program benchmarks
  - Performance regression testing
  - Optimization effectiveness analysis

## 9. Expected Performance Impact

### 9.1 Quantified Improvements by Phase

**After Phase 1 (Foundation)**:
- Baseline measurement capability
- 0% performance improvement (measurement only)

**After Phase 2 (Frontend Optimizations)**:
- Constant folding: 15-25% cycle reduction in arithmetic-heavy code
- Dead code elimination: 5-15% code size reduction
- Loop optimization: 10-30% improvement in loop performance

**After Phase 3 (Register Optimization)**:
- Register spills: 40-60% reduction
- Memory accesses: 20-40% reduction from eliminated spills
- Function call overhead: 15-25% improvement

**After Phase 4 (TSMC Enhancement)**:
- Function call performance: 20-50% improvement with patching
- Parameter passing: Near-zero overhead for constant arguments
- Return value handling: 30-70% faster for immediate use patterns

**After Phase 5 (Backend Optimization)**:
- Instruction selection: 10-20% cycle improvements
- Branch optimization: 15-30% better control flow
- Z80-specific patterns: 5-15% additional gains

**After Phase 6 (Integration & Tuning)**:
- **Combined multiplicative effect: 60-85% overall improvement**
- Code size: 20-40% smaller due to better instruction selection
- Compilation time: <10% increase despite aggressive optimization

### 9.2 Real-World Performance Projections

```
Benchmark Category          Baseline    Optimized   Improvement
Matrix Multiplication:      1,200ms     420ms       65% faster  
Graphics Pixel Processing:  800ms       280ms       65% faster
Audio Sample Processing:    2,100ms     750ms       64% faster
Game Logic Updates:         450ms       180ms       60% faster
Memory Block Operations:    650ms       230ms       65% faster

Average Improvement Across All Benchmarks: 64% faster
```

### 9.3 Memory Usage Optimization

```
Optimization Type           Memory Reduction
Dead Code Elimination:      10-25% code size
Constant Propagation:       5-15% data section
Register Optimization:      20-40% stack usage
Peephole Patterns:          15-30% instruction density
TSMC Templates:            +5-10% (patch points overhead)

Net Memory Usage:          20-35% reduction overall
```

## 10. Conclusion

This comprehensive multi-level optimization guide provides a systematic roadmap for transforming MinZ into the world's most advanced 8-bit compiler. The strategy combines cutting-edge compiler techniques with novel approaches specifically designed for resource-constrained systems.

### 10.1 Revolutionary Achievements

1. **World's First Zero-Overhead Abstractions on 8-bit Hardware**
   - Modern language features with assembly-level performance
   - Functional programming paradigms viable on Z80

2. **TSMC Innovation** 
   - True Self-Modifying Code with instruction patching
   - Runtime code morphing for optimal performance
   - Single-byte patches saving 20+ T-states per optimization

3. **Multiplicative Optimization Effect**
   - Individual optimizations compound for massive gains
   - 60-85% performance improvement achievable
   - Competitive with hand-optimized assembly

### 10.2 Impact on Retro Computing

This optimization system positions MinZ as a **game-changer** for the retro computing community:

- **Enables Modern Programming**: Complex algorithms become feasible on vintage hardware
- **Preserves Performance**: No compromise between expressiveness and speed  
- **Educational Value**: Demonstrates advanced compiler techniques on accessible hardware
- **Community Growth**: Attracts both retro enthusiasts and modern developers

### 10.3 Technical Innovation

The combination of **register pressure optimization** + **TSMC instruction patching** + **comprehensive AST/MIR transformations** creates a unique optimization paradigm that:

- Minimizes resource contention through intelligent scheduling
- Eliminates runtime overhead through compile-time computation
- Adapts code behavior through self-modification
- Leverages Z80 architecture strengths while mitigating weaknesses

### 10.4 Future Potential

This foundation enables future innovations:
- **Cross-architecture support**: Adapt techniques to 6502, 68000, etc.
- **AI-guided optimization**: Machine learning for pattern discovery
- **Dynamic optimization**: Runtime feedback for patch point selection
- **IDE integration**: Real-time performance visualization

---

**MinZ with this optimization system becomes the first compiler to truly bring the modern programming experience to vintage 8-bit systems without performance penalties - a breakthrough that redefines what's possible in retro computing!** ðŸš€

*Document ID: 149*  
*Status: Implementation Ready*  
*Priority: Revolutionary Impact*